{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MNtXcLyyfkg",
        "outputId": "6c72d20e-632a-44d0-9544-986cf2f1a288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "import numpy as np \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower() # Lowercase\n",
        "    text = re.sub(r'[^\\w\\s]',' ',text) # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text) # Remove extra spaces\n",
        "    translator = str.maketrans('', '', '_%')\n",
        "    text = text.translate(translator)\n",
        "    return text.strip()\n",
        "def lemmertize(texts):\n",
        "   #texts input type: list of string\n",
        "   wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "   lemmertize_texts = []\n",
        "   \n",
        "   for text in texts:\n",
        "    words = preprocess(text).split(' ')\n",
        "    lemmertize_texts.append(' '.join([wordnet_lemmatizer.lemmatize(word,pos ='v') for word in words]))\n",
        "\n",
        "   #return lemmertized texts\n",
        "   return lemmertize_texts\n",
        "class Topic_Allocate():\n",
        "  def __init__(self):\n",
        "    self = self\n",
        "\n",
        "  def cbow_fit (self, text_data, window_size = 4):\n",
        "    texts = text_data\n",
        "    #split into words\n",
        "    texts = [text.split() for text in texts]\n",
        "\n",
        "    #embeddind words\n",
        "    word2vec = Word2Vec(texts, min_count = 1, window =  window_size, size = self.vector_size)\n",
        "\n",
        "    # create dictionar\n",
        "    self.dictionary = sorted(list(word2vec.wv.vocab))\n",
        "    self.w2v = word2vec.wv\n",
        "\n",
        "  def cbow_w2v(self, word):\n",
        "    try:\n",
        "      return self.w2v[word]\n",
        "    except:\n",
        "      return np.zeros(self.vector_size)  \n",
        "\n",
        "  def doc2vec (self, text_data, window_size = 4, vector_size = 200, segment_size = 10, data_enrichment = 1, fit = False):\n",
        "      \n",
        "    self.segment_size = segment_size\n",
        "    #lemmertize texts \n",
        "    texts = lemmertize(text_data)\n",
        "    \n",
        "    #lean vocabulary\n",
        "    if fit:\n",
        "      self.vector_size = vector_size\n",
        "      self.cbow_fit(texts, window_size)\n",
        "\n",
        "    #calculate tf_idf\n",
        "    vectorizer = TfidfVectorizer(token_pattern= r'([a-zA-Z0-9µl½¼ménièreºfü]{1,})')\n",
        "    vector = vectorizer.fit_transform(texts)\n",
        "    \n",
        "    #transform texts into matrixs\n",
        "    # ts2vec = np.zeros((len(texts), text_matrix_size, self.vector_size))\n",
        "    doc2vec = []\n",
        "    for idx,text in enumerate(texts):\n",
        "      #get vocab in text and sort alphabetically\n",
        "      words = sorted(list(set(text.split())))\n",
        "      words = np.array(words, dtype = type('a'))\n",
        "      \n",
        "      #cbow_matrix with each row correspond to each word in cbow vector form\n",
        "      cbow_matrix = np.array([self.cbow_w2v(word) for word in words])\n",
        "\n",
        "      #calculate tf_idf \n",
        "      text_vector = np.array(vectorizer.transform([text]).todense().tolist()[0])\n",
        "     \n",
        "      \n",
        "      #remove zero entries\n",
        "      text_vector  = text_vector[text_vector != 0]\n",
        "  \n",
        "      #combine tf_idf with cbow by multiply each cbow vector by its tf_idf\n",
        "      \n",
        "      cbow_tfidf_matrix = np.diag(text_vector) @ cbow_matrix\n",
        "      #remove zero rows\n",
        "      cbow_tfidf_matrix = cbow_tfidf_matrix[np.any(cbow_tfidf_matrix, axis = 1)]\n",
        "  \n",
        "      #compress words into segments \n",
        "      n = cbow_tfidf_matrix.shape[0]\n",
        "      if data_enrichment > segment_size:\n",
        "        data_enrichment = 1\n",
        "        print('data_enrichment cannot be greater than segment_size')\n",
        "      \n",
        "      if n == 0:\n",
        "        t2v = np.zeros((1,self.vector_size))\n",
        "      elif n <= segment_size:\n",
        "        t2v = np.mean(cbow_tfidf_matrix[ : n], axis = 0).reshape(1, self.vector_size)\n",
        "      else:\n",
        "        step = int(segment_size / data_enrichment)\n",
        "        end = n - segment_size\n",
        "        t2v = np.vstack([np.mean(cbow_tfidf_matrix[i : i + segment_size], axis = 0) for i in range(0, end, step)])\n",
        "      \n",
        "        #adjust rows remaining at the end of the matrix \n",
        "        if (n % segment_size) != 0:\n",
        "          t2v = np.vstack((t2v, np.mean(cbow_tfidf_matrix[n - (n % segment_size) : n], axis = 0)))\n",
        "      doc2vec.append(t2v)\n",
        "     \n",
        "    return doc2vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMnp5FsnF0g-"
      },
      "source": [
        "# Medical Notes Classification\n",
        "\n",
        "Medical notes is an useful information source for patient data extraction. Notes classification is also an important task in Medical NLP domain. There are many techniques to solve this problem ranging from traditional method (Logistic Regression, SVM,...) to the state-of-the-art models (Transformer).\n",
        "\n",
        "The below code block is the baseline model for a text classification problem in medical domain.\n",
        "\n",
        "* Input: the corpus of medical transcriptions.\n",
        "* Output: the type of each notes.\n",
        "\n",
        "In this problem, we try to classify five labels:\n",
        "* Surgery\n",
        "* Consult - History and Phy.\n",
        "* Cardiovascular / Pulmonary\n",
        "* Orthopedic\n",
        "* Others\n",
        "\n",
        "The train-test split was also defined, please don't change our split.\n",
        "\n",
        "Metric to evaluate: `f1_macro`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gek0LiL_ukWI"
      },
      "source": [
        "# Baseline Model Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yegXlNOpyegx"
      },
      "source": [
        "\n",
        "0.3729330560342061\n",
        "\n",
        "                                precision    recall  f1-score   support\n",
        "\n",
        "    Cardiovascular / Pulmonary       0.35      0.39      0.37       148\n",
        "    Consult - History and Phy.       0.32      0.06      0.10       207\n",
        "                    Orthopedic       0.39      0.14      0.21       142\n",
        "                         Other       0.66      0.74      0.70      1055\n",
        "                       Surgery       0.43      0.57      0.49       435\n",
        "\n",
        "                      accuracy                           0.56      1987\n",
        "                     macro avg       0.43      0.38      0.37      1987\n",
        "                  weighted avg       0.54      0.56      0.53      1987"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKiIH3ii5XnK"
      },
      "source": [
        "# Library & Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dicxqovc5T3N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "\n",
        "### PLEASE DON'T CHANGE ANYTHING IN THIS SECTION ###\n",
        "DATA = \"https://github.com/socd06/private_nlp/raw/master/data/mtsamples.csv\"\n",
        "\n",
        "filtered_labels = [\n",
        "    \"Surgery\",\n",
        "    \"Consult - History and Phy.\",\n",
        "    \"Cardiovascular / Pulmonary\",\n",
        "    \"Orthopedic\",\n",
        "]\n",
        "data = pd.read_csv(DATA, usecols=['medical_specialty', 'transcription']).dropna()\n",
        "data.columns = ['labels', 'text']\n",
        "data['labels'] = [i.strip() if (i.strip() in filtered_labels) else 'Other' for i in data.labels.to_list()]\n",
        "train, test = train_test_split(data, test_size=0.4, stratify=data.labels, random_state=0)\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "### END ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uquVeF7Z-cBG"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAh6AQjmVMwu"
      },
      "source": [
        "# My Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h7loILJeHufc"
      },
      "outputs": [],
      "source": [
        "#encode labels \n",
        "le = LabelEncoder()\n",
        "train['labels'] = le.fit_transform(train.labels)\n",
        "test['labels'] = le.transform(test.labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyUT3xshkV0W"
      },
      "source": [
        "### Encode Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U7sAJD9Myeg8"
      },
      "outputs": [],
      "source": [
        "#create model\n",
        "model = Topic_Allocate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgsuAOLDkemn",
        "outputId": "e87e12ef-b5bd-4509-94e5-eb50dee7dca3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        }
      ],
      "source": [
        "vector_size = 500\n",
        "segment_size = 20\n",
        "data_enrichment = 3\n",
        "#encode texts into matrix\n",
        "X_train = np.asarray(model.doc2vec(train['text'], vector_size = vector_size, segment_size = segment_size, data_enrichment = data_enrichment, fit = True))\n",
        "X_test = np.asarray(model.doc2vec(test['text'], vector_size = vector_size, segment_size = segment_size, data_enrichment = data_enrichment))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8vRdt7T2kz2",
        "outputId": "36685aa8-3223-4692-eb90-6b21b2e1601f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Consult - History and Phy.']\n"
          ]
        }
      ],
      "source": [
        "# onehot labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "y_train = train['labels']\n",
        "\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "Y_train = onehot_encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "Y_test = onehot_encoder.transform(np.array(test['labels']).reshape(-1,1))\n",
        "\n",
        "# invert first example\n",
        "inverted = le.inverse_transform([np.argmax(Y_train[0, :])])\n",
        "print(inverted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_azl76XH2njr"
      },
      "source": [
        "### Train with LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ydJ1v6byehB"
      },
      "source": [
        "#### sequence -> vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yc5F_LUTyehB"
      },
      "outputs": [],
      "source": [
        "#%% import library\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "# from keras.metrics.cate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jz6v3dxXyehC"
      },
      "outputs": [],
      "source": [
        "max_size = np.amax(np.array([x.shape[0] for x in X_train]))\n",
        "def fill_zeros(x, Vector_size):\n",
        "    try:\n",
        "        missing = max_size - x.shape[0]\n",
        "        fill_in = np.zeros((missing, Vector_size))\n",
        "        return np.vstack((fill_in, x))\n",
        "    except:\n",
        "        return np.zeros((max_size, Vector_size))\n",
        "func = lambda x: fill_zeros(x, 500)\n",
        "X_train_lstm_s2v = np.array([func(x) for x in X_train])\n",
        "X_test_lstm_s2v = np.array([func(x) for x in X_test])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDAEFLpiyehG"
      },
      "source": [
        "## Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9gLXgIA-yehH"
      },
      "outputs": [],
      "source": [
        "# pytorch mlp for regression\n",
        "# from numpy import vstack\n",
        "# from numpy import sqrt\n",
        "# from pandas import read_csv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch import Tensor, nn\n",
        "# from torch.optim import SGD\n",
        "# from torch.nn import MSELoss\n",
        "# from torch.nn.init import xavier_uniform_\n",
        "from torch import optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import math\n",
        "import keras.backend as K\n",
        "# import tensorflow as tf\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WGhcdiglyehI"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "import torch\n",
        "class talosix_dataset(data.Dataset):\n",
        "    def __init__(self, text_data, text_label):\n",
        "        super().__init__()\n",
        "        #text_data is a np.ndarray\n",
        "        self.size = text_data.shape[0]\n",
        "        self.data = torch.from_numpy(np.double(text_data))\n",
        "        self.label = torch.from_numpy(np.double(text_label))\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_point = self.data[idx]\n",
        "        data_label = self.label[idx]\n",
        "        return data_point, data_label\n",
        "\n",
        "train_set = talosix_dataset(X_train_lstm_s2v, Y_train)\n",
        "test_set = talosix_dataset(X_test_lstm_s2v, Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esLU7NPxzBAf",
        "outputId": "378b0904-65b1-4dc0-c142-4f53f2afdbee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using torch 1.9.0+cu111\n",
            "Is the GPU available? True\n",
            "Device cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"Using torch\", torch.__version__)\n",
        "torch.manual_seed(42)  # Setting the seed\n",
        "gpu_avail = torch.cuda.is_available()\n",
        "print(f\"Is the GPU available? {gpu_avail}\")\n",
        "device = torch.device(\"cuda\") \n",
        "\n",
        "print(\"Device\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VsKSvJEUyehI"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable \n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, num_classes, input_size, num_layers):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.num_classes = num_classes #number of classes\n",
        "        self.num_layers = num_layers   #number of layers\n",
        "        self.input_size = input_size   #input size\n",
        "        self.hidden_size = 50          #hidden state\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels = vector_size, out_channels = vector_size, kernel_size = 3, padding = 1, stride = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(vector_size),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout(0.2),\n",
        "            # nn.Conv1d(in_channels = vector_size, out_channels = vector_size, kernel_size = 3, padding = 1, stride = 1),\n",
        "            # nn.MaxPool1d(2)\n",
        "        )\n",
        "\n",
        "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size = self.hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True, dropout = 0.2) #lstm\n",
        "\n",
        "        self.lstm2 =  nn.LSTM(input_size = self.hidden_size, hidden_size = 50, batch_first = True, dropout = 0.2)\n",
        "        self.fc1 = nn.Linear(50, num_classes) #fully connected last layer\n",
        "    \n",
        "    def forward(self, x):\n",
        "        inp = torch.moveaxis(x, 1, 2)\n",
        "        inp = self.cnn(inp)\n",
        "        inp = torch.moveaxis(inp, 1, 2)\n",
        "\n",
        "        self.out1, (hn1, cn1) = self.lstm1(inp) #lstm with input, hidden, and internal state\n",
        "        self.out2, (hn2, cn2) = self.lstm2(self.out1)\n",
        "\n",
        "        hn2 = hn2.view(-1, 50) #reshaping the data for Dense layer next\n",
        "        out = self.relu(hn2)\n",
        "        out = self.fc1(out) #first Dense\n",
        "        out = self.softmax(out) #relu\n",
        "\n",
        "        return out "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jzomYjWtiUqF"
      },
      "outputs": [],
      "source": [
        "m = nn.Dropout(p=0.2)\n",
        "input = torch.randn(20, 16)\n",
        "output = m(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hZtYzYBz5eGA"
      },
      "outputs": [],
      "source": [
        "class f1_loss(nn.Module):\n",
        "  def __init__(self, weight = None):\n",
        "    super(f1_loss, self).__init__()\n",
        "    self.weight = weight\n",
        "\n",
        "\n",
        "  def forward(self, y_pred, y_true):\n",
        "\n",
        "    tp = torch.sum(y_true*y_pred, dim=0)\n",
        "    # tn = torch.sum((1-y_true)*(1-y_pred), dim=0)\n",
        "    fp = torch.sum((1-y_true)*y_pred, dim=0)\n",
        "    fn = torch.sum(y_true*(1-y_pred), dim=0)\n",
        "\n",
        "    p = tp / (tp + fp + 1e-10)\n",
        "    r = tp / (tp + fn + 1e-10)\n",
        "\n",
        "    f1 = 2*p*r / (p+r+1e-10)\n",
        "    f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1), f1)\n",
        "    if self.weight != None:\n",
        "      f1 = f1 * torch.Tensor(self.weight).cuda()\n",
        "\n",
        "    return 1 - torch.mean(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Txqsx9FAyehJ"
      },
      "outputs": [],
      "source": [
        "def train (num_epochs, model, loaders, loss_func, lr, wd = 0):\n",
        "    model.train()\n",
        "\n",
        "    total_step = len(loaders)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (x, y) in enumerate(loaders):\n",
        "\n",
        "            x, y = x.type(torch.float).to('cuda'), y.type(torch.float).to('cuda')\n",
        "            \n",
        "            out = model(x)\n",
        "            loss = loss_func(out, y)\n",
        "\n",
        "            # optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "            optimizer = optim.Adam(model.parameters(), lr = lr/(2**(epoch//10)), weight_decay = wd)\n",
        "            # optimizer = optim.Adam(model.parameters(), lr = lr/(epoch//10 + 1))\n",
        "            # optimizer = optim.Adam(model.parameters(), lr = lr/((epoch+1)//math.sqrt( 0.6 * (epoch + 1)  )))\n",
        "\n",
        "\n",
        "            # optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % total_step == 0:\n",
        "                Y_pred = model(torch.Tensor(X_train_lstm_s2v).to('cuda'))\n",
        "                check1 = torch.argmax(Y_pred, dim = 1, keepdim= True).cpu()\n",
        "                ytrain1 = np.argmax(Y_train, axis = 1)\n",
        "\n",
        "                Y_pred = model(torch.Tensor(X_test_lstm_s2v).to('cuda'))\n",
        "                check2 = torch.argmax(Y_pred, dim = 1, keepdim= True).cpu()\n",
        "                ytrain2 = np.argmax(Y_test, axis = 1)\n",
        "\n",
        "                \n",
        "\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, f1: {:.4f}, val_f1: {:.4f}' \n",
        "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), metrics.f1_score(ytrain1, check1, average='macro'), metrics.f1_score(ytrain2, check2, average='macro')))\n",
        "                del(Y_pred, check1,ytrain1, check2, ytrain2)\n",
        "                pass\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    \n",
        "    pass\n",
        "\n",
        "            \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qACJhZK7yehK"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle = True)\n",
        "test_dataloader = DataLoader(test_set, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xyV0vuS1CMLS"
      },
      "outputs": [],
      "source": [
        "loss_fn = f1_loss(weight = [0.32, 0.22, 0.32, 0.04, 0.1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vVM8JPwEopD",
        "outputId": "868bda5b-f385-4104-c32c-4ed6ac63fb35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Step [187/187], Loss: 0.9989, f1: 0.3107, val_f1: 0.3420\n",
            "Epoch [2/30], Step [187/187], Loss: 0.9890, f1: 0.3999, val_f1: 0.4297\n",
            "Epoch [3/30], Step [187/187], Loss: 0.9052, f1: 0.4126, val_f1: 0.4122\n",
            "Epoch [4/30], Step [187/187], Loss: 0.9821, f1: 0.3660, val_f1: 0.3684\n",
            "Epoch [5/30], Step [187/187], Loss: 0.9276, f1: 0.3893, val_f1: 0.4153\n",
            "Epoch [6/30], Step [187/187], Loss: 0.9761, f1: 0.4282, val_f1: 0.4493\n",
            "Epoch [7/30], Step [187/187], Loss: 0.9742, f1: 0.4576, val_f1: 0.4667\n",
            "Epoch [8/30], Step [187/187], Loss: 0.9355, f1: 0.3797, val_f1: 0.3741\n",
            "Epoch [9/30], Step [187/187], Loss: 0.9966, f1: 0.4284, val_f1: 0.4431\n",
            "Epoch [10/30], Step [187/187], Loss: 0.9923, f1: 0.3964, val_f1: 0.4117\n",
            "Epoch [11/30], Step [187/187], Loss: 0.9867, f1: 0.4628, val_f1: 0.4758\n",
            "Epoch [12/30], Step [187/187], Loss: 0.9441, f1: 0.4786, val_f1: 0.4982\n",
            "Epoch [13/30], Step [187/187], Loss: 0.9902, f1: 0.4469, val_f1: 0.4517\n",
            "Epoch [14/30], Step [187/187], Loss: 0.9722, f1: 0.4679, val_f1: 0.4684\n",
            "Epoch [15/30], Step [187/187], Loss: 0.9561, f1: 0.4540, val_f1: 0.4359\n",
            "Epoch [16/30], Step [187/187], Loss: 0.9953, f1: 0.4743, val_f1: 0.4646\n",
            "Epoch [17/30], Step [187/187], Loss: 0.9946, f1: 0.4756, val_f1: 0.4646\n",
            "Epoch [18/30], Step [187/187], Loss: 0.9720, f1: 0.4847, val_f1: 0.4921\n",
            "Epoch [19/30], Step [187/187], Loss: 0.9948, f1: 0.4827, val_f1: 0.4776\n",
            "Epoch [20/30], Step [187/187], Loss: 0.9152, f1: 0.4828, val_f1: 0.4728\n",
            "Epoch [21/30], Step [187/187], Loss: 0.9997, f1: 0.5037, val_f1: 0.4866\n",
            "Epoch [22/30], Step [187/187], Loss: 0.9191, f1: 0.5037, val_f1: 0.4830\n",
            "Epoch [23/30], Step [187/187], Loss: 0.9724, f1: 0.5057, val_f1: 0.4783\n",
            "Epoch [24/30], Step [187/187], Loss: 0.9960, f1: 0.4980, val_f1: 0.4739\n",
            "Epoch [25/30], Step [187/187], Loss: 0.9981, f1: 0.4840, val_f1: 0.4576\n",
            "Epoch [26/30], Step [187/187], Loss: 0.9918, f1: 0.4696, val_f1: 0.4664\n",
            "Epoch [27/30], Step [187/187], Loss: 0.9861, f1: 0.4954, val_f1: 0.4579\n",
            "Epoch [28/30], Step [187/187], Loss: 0.9994, f1: 0.5084, val_f1: 0.4801\n",
            "Epoch [29/30], Step [187/187], Loss: 0.9987, f1: 0.5148, val_f1: 0.4689\n",
            "Epoch [30/30], Step [187/187], Loss: 0.9075, f1: 0.5074, val_f1: 0.4730\n"
          ]
        }
      ],
      "source": [
        "model = LSTM(5, 500, 1).cuda()\n",
        "train(20, model, train_dataloader, loss_fn, 0.001, 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDa7sOnQJTUH",
        "outputId": "3fa0b474-250c-4f3a-ac71-1f4c1abf9794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 66  19   0  85  32]\n",
            " [  2  71  20 123   1]\n",
            " [  0   3  80  53  76]\n",
            " [ 24 114  22 501  11]\n",
            " [ 56   0  20 293 315]]\n",
            "0.46144509456116206\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "Cardiovascular / Pulmonary       0.33      0.45      0.38       148\n",
            "Consult - History and Phy.       0.33      0.34      0.33       207\n",
            "                Orthopedic       0.38      0.56      0.45       142\n",
            "                     Other       0.75      0.47      0.58      1055\n",
            "                   Surgery       0.46      0.72      0.56       435\n",
            "\n",
            "                  accuracy                           0.52      1987\n",
            "                 macro avg       0.45      0.51      0.46      1987\n",
            "              weighted avg       0.58      0.52      0.53      1987\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ],
      "source": [
        "Y_pred = model(torch.Tensor(X_test_lstm_s2v).to('cuda'))\n",
        "check = torch.argmax(Y_pred, dim = 1, keepdim= True).cpu()\n",
        "ytrain = np.argmax(Y_test, axis = 1)\n",
        "print(confusion_matrix(check, ytrain))\n",
        "print(metrics.f1_score(ytrain, check, average='macro'))\n",
        "print(metrics.classification_report(ytrain, check, target_names=list(le.classes_)))\n",
        "del(Y_pred, check, ytrain)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Db92OTFOvUU"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhI7UGksOy9I"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "model = LSTM(5, 500, 1).cuda()\n",
        "summary(model, (2979, 121, 500))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "PRJ.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
